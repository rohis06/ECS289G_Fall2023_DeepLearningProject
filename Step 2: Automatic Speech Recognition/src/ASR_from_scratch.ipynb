{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ECS 289G: Automatic Speech Recognition using Convolutional Neural Network-Recurrent Neural Network (CNN-RNN)"
      ],
      "metadata": {
        "id": "1iiwHMAtLp1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook I have implemented the task of Automatic Speech Recognition (ASR) using Convolutional Neural Network-Recurrent Neural Network architecture. The model is being trained on the LJ Speech dataset."
      ],
      "metadata": {
        "id": "FJzYMjFRTJ7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mltu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlN5Wk0uoe78",
        "outputId": "9bcc0e30-8a29-4b2a-fe3f-c6424aa9ba2c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mltu\n",
            "  Downloading mltu-1.1.7-py3-none-any.whl (45 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=6.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (6.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from mltu) (4.66.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from mltu) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mltu) (1.23.5)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from mltu) (4.8.0.76)\n",
            "Requirement already satisfied: Pillow>=9.4.0 in /usr/local/lib/python3.10/dist-packages (from mltu) (9.4.0)\n",
            "Collecting onnxruntime>=1.15.0 (from mltu)\n",
            "  Downloading onnxruntime-1.16.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m80.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mltu) (3.7.1)\n",
            "Collecting coloredlogs (from onnxruntime>=1.15.0->mltu)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (23.5.26)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (23.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (3.20.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime>=1.15.0->mltu) (1.12)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mltu) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->mltu) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mltu) (1.16.0)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.15.0->mltu)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime>=1.15.0->mltu) (1.3.0)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime, mltu\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 mltu-1.1.7 onnxruntime-1.16.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the libraries\n",
        "import os\n",
        "import tarfile\n",
        "from tqdm import tqdm\n",
        "from urllib.request import urlopen\n",
        "from io import BytesIO"
      ],
      "metadata": {
        "id": "NgXoD9jlHzeS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Downloading and unzipping the LJSpeech Dataset\n",
        "def download_and_unzip(url, extract_to='/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Datasets', chunk_size=1024*1024):\n",
        "    http_response = urlopen(url)\n",
        "\n",
        "    data = b''\n",
        "    iterations = http_response.length // chunk_size + 1\n",
        "    for _ in tqdm(range(iterations)):\n",
        "        data += http_response.read(chunk_size)\n",
        "\n",
        "    os.makedirs(extract_to, exist_ok=True)\n",
        "\n",
        "    tarFile = tarfile.open(fileobj=BytesIO(data), mode='r|bz2')\n",
        "    tarFile.extractall(path=extract_to)\n",
        "    tarFile.close()\n",
        "\n",
        "dataset_path = '/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Datasets/LJSpeech-1.1'\n",
        "if not os.path.exists(dataset_path):\n",
        "    download_and_unzip('https://data.keithito.com/data/speech/LJSpeech-1.1.tar.bz2', extract_to='/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Datasets')"
      ],
      "metadata": {
        "id": "0meHNdB4zBEP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the model configs\n",
        "from datetime import datetime\n",
        "\n",
        "from mltu.configs import BaseModelConfigs\n",
        "\n",
        "class ModelConfigs(BaseModelConfigs):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model_path = os.path.join(\"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Models/05_sound_to_text\", datetime.strftime(datetime.now(), \"%Y%m%d%H%M\"))\n",
        "        self.frame_length = 256\n",
        "        self.frame_step = 160\n",
        "        self.fft_length = 384\n",
        "\n",
        "        self.vocab = \"abcdefghijklmnopqrstuvwxyz'?! \"\n",
        "        self.input_shape = None\n",
        "        self.max_text_length = None\n",
        "        self.max_spectrogram_length = None\n",
        "\n",
        "        self.batch_size = 8\n",
        "        self.learning_rate = 0.0005\n",
        "        self.train_epochs = 100\n",
        "        self.train_workers = 20"
      ],
      "metadata": {
        "id": "0yyRr2HmpI1Z"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the model configs\n",
        "import pandas as pd\n",
        "from mltu.preprocessors import WavReader\n",
        "\n",
        "dataset_path = \"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Datasets/LJSpeech-1.1\"\n",
        "metadata_path = dataset_path + \"/metadata.csv\"\n",
        "wavs_path = dataset_path + \"/wavs/\"\n",
        "\n",
        "metadata_df = pd.read_csv(metadata_path, sep=\"|\", header=None, quoting=3)\n",
        "metadata_df.columns = [\"file_name\", \"transcription\", \"normalized_transcription\"]\n",
        "metadata_df = metadata_df[[\"file_name\", \"normalized_transcription\"]]\n",
        "\n",
        "dataset = [[f\"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Datasets/LJSpeech-1.1/wavs/{file}.wav\", label] for file, label in metadata_df.values.tolist()]\n",
        "\n",
        "# Create a ModelConfigs object to store model configurations\n",
        "configs = ModelConfigs()\n",
        "\n",
        "max_text_length, max_spectrogram_length = 0, 0\n",
        "for file_path, label in tqdm(dataset):\n",
        "    spectrogram = WavReader.get_spectrogram(file_path, frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n",
        "    valid_label = [c for c in label.lower() if c in configs.vocab]\n",
        "    max_text_length = max(max_text_length, len(valid_label))\n",
        "    max_spectrogram_length = max(max_spectrogram_length, spectrogram.shape[0])\n",
        "    configs.input_shape = [max_spectrogram_length, spectrogram.shape[1]]\n",
        "\n",
        "configs.max_spectrogram_length = max_spectrogram_length\n",
        "configs.max_text_length = max_text_length\n",
        "configs.save()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NyK7XuklHpc",
        "outputId": "e4b933a5-e5d7-49dc-9bee-2c8d5ed93e11"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13100/13100 [1:49:45<00:00,  1.99it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing mltu specific libraries\n",
        "from mltu.tensorflow.dataProvider import DataProvider\n",
        "from mltu.transformers import LabelIndexer, LabelPadding, SpectrogramPadding\n",
        "from mltu.tensorflow.losses import CTCloss\n",
        "from mltu.tensorflow.callbacks import Model2onnx, TrainLogger\n",
        "from mltu.tensorflow.metrics import CERMetric, WERMetric"
      ],
      "metadata": {
        "id": "ojtxuU95rH30"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a data provider for the LJ speech dataset\n",
        "data_provider = DataProvider(\n",
        "    dataset=dataset,\n",
        "    skip_validation=True,\n",
        "    batch_size=configs.batch_size,\n",
        "    data_preprocessors=[\n",
        "        WavReader(frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length),\n",
        "        ],\n",
        "    transformers=[\n",
        "        SpectrogramPadding(max_spectrogram_length=configs.max_spectrogram_length, padding_value=0),\n",
        "        LabelIndexer(configs.vocab),\n",
        "        LabelPadding(max_word_length=configs.max_text_length, padding_value=len(configs.vocab)),\n",
        "        ],\n",
        ")"
      ],
      "metadata": {
        "id": "GLFXKXmdq4pm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf563461-1869-410b-c994-2ccd1c01abf0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:DataProvider:Skipping Dataset validation...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the dataset into training and validation sets\n",
        "train_data_provider, val_data_provider = data_provider.split(split = 0.9)"
      ],
      "metadata": {
        "id": "tiCkcnsrrOR6"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras import layers\n",
        "from keras.models import Model\n",
        "\n",
        "from mltu.tensorflow.model_utils import residual_block, activation_layer\n",
        "\n",
        "# The network we have built\n",
        "def train_model(input_dim, output_dim, activation='leaky_relu', dropout=0.2):\n",
        "\n",
        "    inputs = layers.Input(shape=input_dim, name=\"input\")\n",
        "\n",
        "    input = layers.Lambda(lambda x: tf.expand_dims(x, axis=-1))(inputs)\n",
        "\n",
        "    # Convolution layer 1\n",
        "    x = layers.Conv2D(filters=32, kernel_size=[11, 41], strides=[2, 2], padding=\"same\", use_bias=False)(input)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = activation_layer(x, activation='leaky_relu')\n",
        "\n",
        "    # Convolution layer 2\n",
        "    x = layers.Conv2D(filters=32, kernel_size=[11, 21], strides=[1, 2], padding=\"same\", use_bias=False)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = activation_layer(x, activation='leaky_relu')\n",
        "\n",
        "    # Reshaping the resulted volume to feed the RNNs layers\n",
        "    x = layers.Reshape((-1, x.shape[-2] * x.shape[-1]))(x)\n",
        "\n",
        "    # RNN layers\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True))(x)\n",
        "\n",
        "    # Dense layer\n",
        "    x = layers.Dense(256)(x)\n",
        "    x = activation_layer(x, activation='leaky_relu')\n",
        "    x = layers.Dropout(dropout)(x)\n",
        "\n",
        "    # Classification layer\n",
        "    output = layers.Dense(output_dim + 1, activation=\"softmax\")(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=output)\n",
        "    return model"
      ],
      "metadata": {
        "id": "rIJskh0_x10S"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the tensorflow model\n",
        "model = train_model(\n",
        "    input_dim = configs.input_shape,\n",
        "    output_dim = len(configs.vocab),\n",
        "    dropout=0.5\n",
        ")"
      ],
      "metadata": {
        "id": "1eRcyb1NrQ-i"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the model\n",
        "# I have used Adam optimier and CTCloss for this task\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=configs.learning_rate),\n",
        "    loss=CTCloss(),\n",
        "    metrics=[\n",
        "        CERMetric(vocabulary=configs.vocab),\n",
        "        WERMetric(vocabulary=configs.vocab)\n",
        "        ],\n",
        "    run_eagerly=False\n",
        ")"
      ],
      "metadata": {
        "id": "9JKsMfUa7xu4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tf2onnx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofh6hQAIQJCA",
        "outputId": "e36c2254-29ce-4752-ab1d-a8dc12da9bfd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf2onnx\n",
            "  Downloading tf2onnx-1.15.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.7/454.7 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.1 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.23.5)\n",
            "Collecting onnx>=1.4.1 (from tf2onnx)\n",
            "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (23.5.26)\n",
            "Requirement already satisfied: protobuf~=3.20.2 in /usr/local/lib/python3.10/dist-packages (from tf2onnx) (3.20.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->tf2onnx) (2023.11.17)\n",
            "Installing collected packages: onnx, tf2onnx\n",
            "Successfully installed onnx-1.15.0 tf2onnx-1.15.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
        "\n",
        "# Defining callbacks\n",
        "earlystopper = EarlyStopping(monitor='val_CER', patience=20, verbose=1, mode='min')\n",
        "checkpoint = ModelCheckpoint(f\"{configs.model_path}/model.h5\", monitor='val_CER', verbose=1, save_best_only=True, mode='min')\n",
        "trainLogger = TrainLogger(configs.model_path)\n",
        "tb_callback = TensorBoard(f'{configs.model_path}/logs', update_freq=1)\n",
        "reduceLROnPlat = ReduceLROnPlateau(monitor='val_CER', factor=0.8, min_delta=1e-10, patience=5, verbose=1, mode='auto')\n",
        "model2onnx = Model2onnx(f\"{configs.model_path}/model.h5\")\n",
        "\n",
        "# Training the model\n",
        "model.fit(\n",
        "    train_data_provider,\n",
        "    validation_data=val_data_provider,\n",
        "    epochs=configs.train_epochs,\n",
        "    callbacks=[earlystopper, checkpoint, trainLogger, reduceLROnPlat, tb_callback, model2onnx],\n",
        "    workers=configs.train_workers\n",
        ")\n",
        "\n",
        "# Saving training and validation datasets as csv files\n",
        "train_data_provider.to_csv(os.path.join(configs.model_path, 'train.csv'))\n",
        "val_data_provider.to_csv(os.path.join(configs.model_path, 'val.csv'))"
      ],
      "metadata": {
        "id": "7Txd_mKU70e_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing it on validation dataset of LJ Speech and obtaining the Average CER and WER\n",
        "import typing\n",
        "import numpy as np\n",
        "\n",
        "from mltu.inferenceModel import OnnxInferenceModel\n",
        "from mltu.preprocessors import WavReader\n",
        "from mltu.utils.text_utils import ctc_decoder, get_cer, get_wer\n",
        "\n",
        "class WavToTextModel(OnnxInferenceModel):\n",
        "    def __init__(self, char_list: typing.Union[str, list], *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.char_list = char_list\n",
        "\n",
        "    def predict(self, data: np.ndarray):\n",
        "        data_pred = np.expand_dims(data, axis=0)\n",
        "\n",
        "        preds = self.model.run(None, {self.input_name: data_pred})[0]\n",
        "\n",
        "        text = ctc_decoder(preds, self.char_list)[0]\n",
        "\n",
        "        return text\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    from tqdm import tqdm\n",
        "    from mltu.configs import BaseModelConfigs\n",
        "\n",
        "    configs = BaseModelConfigs.load(\"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Models/05_sound_to_text/20231128/configs.yaml\")\n",
        "\n",
        "    model = WavToTextModel(model_path=configs.model_path, char_list=configs.vocab, force_cpu=False)\n",
        "\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Models/05_sound_to_text/20231128/val.csv\").values.tolist()\n",
        "\n",
        "    accum_cer, accum_wer = [], []\n",
        "    for wav_path, label in tqdm(df):\n",
        "\n",
        "        spectrogram = WavReader.get_spectrogram(wav_path, frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n",
        "\n",
        "        padded_spectrogram = np.pad(spectrogram, ((0, configs.max_spectrogram_length - spectrogram.shape[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "\n",
        "        text = model.predict(padded_spectrogram)\n",
        "\n",
        "        true_label = \"\".join([l for l in label.lower() if l in configs.vocab])\n",
        "\n",
        "        cer = get_cer(text, true_label)\n",
        "        wer = get_wer(text, true_label)\n",
        "\n",
        "        accum_cer.append(cer)\n",
        "        accum_wer.append(wer)\n",
        "\n",
        "    print(f\"Average CER: {np.average(accum_cer)}, Average WER: {np.average(accum_wer)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5WFsOv-ecaep",
        "outputId": "45880ba3-5a1a-4052-8c93-7f4f29eed8ed"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1310/1310 [08:16<00:00,  2.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average CER: 0.02472644738965785, Average WER: 0.09590515265714795\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Predicting the transcription for a random audio sample\n",
        "spectrogram = WavReader.get_spectrogram(\"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Datasets/LJSpeech-1.1/wavs/LJ010-0251.wav\", frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n",
        "padded_spectrogram = np.pad(spectrogram, ((0, configs.max_spectrogram_length - spectrogram.shape[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "text = model.predict(padded_spectrogram)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnq_hTw0r4E1",
        "outputId": "209346b2-fd7e-46cf-bbfe-af7ed9dc2b40"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "francis was sentenced to be hanged decapitated end quartered\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing it with custom audios (the audios which we obatined after running speaker diarization and audio splicing)\n",
        "import os\n",
        "\n",
        "directory_path = \"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/Dataset/spliced\"\n",
        "output_file_path = \"/content/drive/MyDrive/Fall 2023/ECS 289G/Project/all_transcriptions_cnn-rnn.txt\"\n",
        "\n",
        "files = []\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    files.append(filename)\n",
        "\n",
        "sorted_files = sorted(files, key=lambda x: int(x.split('_')[-1].split('.')[0]))\n",
        "\n",
        "# Opening the output file once for writing\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    for filename in sorted_files:\n",
        "        if os.path.isfile(os.path.join(directory_path, filename)):\n",
        "            # Predicting the transcriptions for each of the spliced audio files\n",
        "            file_path = os.path.join(directory_path, filename)\n",
        "            spectrogram = WavReader.get_spectrogram(file_path, frame_length=configs.frame_length, frame_step=configs.frame_step, fft_length=configs.fft_length)\n",
        "            padded_spectrogram = np.pad(spectrogram, ((0, configs.max_spectrogram_length - spectrogram.shape[0]), (0, 0)), mode='constant', constant_values=0)\n",
        "            result = model.predict(padded_spectrogram)\n",
        "            speaker_id = filename.split('_')[1]\n",
        "            transcription_text = result\n",
        "\n",
        "            # Writing to the text file\n",
        "            output_file.write(f\"Speaker {speaker_id}: {transcription_text}\\n\")\n",
        "\n",
        "            # Printing the transciption\n",
        "            print(f\"Speaker {speaker_id}: {transcription_text}\\n\")\n",
        "\n",
        "    print(f\"All transcriptions successfully stored in {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQN4WI1np8wl",
        "outputId": "3cc8e8a7-8630-4fb4-88c6-8d8f977ab5e4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/librosa/core/spectrum.py:257: UserWarning: n_fft=384 is too large for input signal of length=375\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Speaker 00: \n",
            "\n",
            "Speaker 00: thrus hosurby lurd\n",
            "\n",
            "Speaker 01: kinohibminave winwoond honmer dloth\n",
            "\n",
            "Speaker 00: ogothgoad yiton mintguorine goded i dirgardoewat miador tha indimidenth bevenbitinoaymabu and now une wooking re micouted froni howard you\n",
            "\n",
            "Speaker 01: i am dues whoe gin on amaenron comb maminuam benop goth would enhumeweyman migament\n",
            "\n",
            "Speaker 00: bone gride thom and the howe the thaughtered on throught moth not hudin fismarded\n",
            "\n",
            "Speaker 00: chould\n",
            "\n",
            "All transcriptions successfully stored in /content/drive/MyDrive/Fall 2023/ECS 289G/Project/all_transcriptions_cnn-rnn.txt\n"
          ]
        }
      ]
    }
  ]
}